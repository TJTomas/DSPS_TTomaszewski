{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIVRXv+dTPzPL7NTu2NKEW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TJTomas/DSPS_TTomaszewski/blob/main/HW9/Homework_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1suivPtPWhx6"
      },
      "outputs": [],
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "\n",
        "import ipywidgets as ipw\n",
        "import numpy as np\n",
        "import pylab as pl\n",
        "import sklearn.cluster\n",
        "from sklearn import mixture\n",
        "from scipy.cluster.vq import kmeans2\n",
        "%pylab inline\n",
        "\n",
        "\n",
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1:\n",
        "Task 1: make a kaggle account and set up your API (see below)"
      ],
      "metadata": {
        "id": "0C40oxG2qZBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this mounts your google drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "\n"
      ],
      "metadata": {
        "id": "P_Z1qX7hYOJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this gets you to your drive folder\n",
        "%cd gdrive/My\\ Drive/"
      ],
      "metadata": {
        "id": "wHSW0kzFpXQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this makes sure the file is there: this cell should return \"kaggle.json\"\n",
        "!ls kaggle.json"
      ],
      "metadata": {
        "id": "F2fXY5aHqMkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this limits who can view and make changes who can access this file.\n",
        "!chmod 600 kaggle.json\n",
        "\n",
        "# this reads in the file and stores it into the system variables of your colab sessions which allows you to connect programmatically to the kaggle platform\n",
        "envs = json.load(open(\"kaggle.json\", \"r\"))\n",
        "os.environ[\"KAGGLE_USERNAME\"] = envs['username']\n",
        "os.environ[\"KAGGLE_KEY\"] = \"e60b57c215e877e01a22375a3058eec1\"#envs['key']"
      ],
      "metadata": {
        "id": "-aGwoI16qR9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 2\n",
        "Task 2: read in the data for the World Happniess Dataset https://www.kaggle.com/datasets/unsdsn/world-happiness/data - use the 2024 data"
      ],
      "metadata": {
        "id": "C7IZXYuoqwBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kaggle"
      ],
      "metadata": {
        "id": "KQ3oIkE8qyXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the kaggle.api.dataset_download_files function to download a specific file from a Kaggle dataset in Jupyter Notebook:\n",
        "#kaggle.api.dataset_download_files(\"unsdsn/world-happiness\", path=\"./\", unzip=True, quiet=False)\n",
        "kaggle.api.dataset_download_files(\"mathurinache/world-happiness-report\", path=\"./\", unzip=True, quiet=False)"
      ],
      "metadata": {
        "id": "xolbeUdmr4ZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "happiness = pd.read_csv(\"2021.csv\")\n",
        "\n",
        "# displaying the 6 raws of contents of the cvs file\n",
        "happiness.head()"
      ],
      "metadata": {
        "id": "ufLnkDyQCYFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "happiness.shape"
      ],
      "metadata": {
        "id": "2X6x-aOXCYnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "happiness.describe()"
      ],
      "metadata": {
        "id": "bkw07Q90Cjxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scaling\n",
        "## Task 3:\n",
        "Task 3: For each numerical column X, prepare a column that is a minmax version of X and a version that is the standardized version of X, store them in the same or another dataframe (your choice) as, for example X_minmax and X_standardized (choose the variable or dataframe names you want, but make them meaningful and descriptive!)"
      ],
      "metadata": {
        "id": "-3y2bdcYColL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define min-max scaling function\n",
        "def min_max_scale(data):\n",
        "    min_val = data.min()\n",
        "    max_val = data.max()\n",
        "    return (data - min_val) / (max_val - min_val)\n",
        "\n",
        "# Define standardization function\n",
        "def standardize(data):\n",
        "    mean_val = data.mean()\n",
        "    std_val = data.std()\n",
        "    return (data - mean_val) / std_val"
      ],
      "metadata": {
        "id": "z-_-1txvCvAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_cols = happiness.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "# Two new DataFrames to store the scaled columns\n",
        "scaled_happiness_min_max = pd.DataFrame()\n",
        "scaled_happiness_std = pd.DataFrame()\n",
        "\n",
        "# Iterate through numerical columns and apply scaling\n",
        "for col in numerical_cols:\n",
        "\n",
        "    # Apply Min-Max scaling\n",
        "    scaled_happiness_min_max[f'{col}_minmax'] = min_max_scale(happiness[col])\n",
        "\n",
        "    # Apply Standardization\n",
        "    scaled_happiness_std[f'{col}_standardized'] = standardize(happiness[col])\n",
        "\n",
        "# Drop the 'Ladder score in Dystopia_minmax' column as it contains NaN / all zeros\n",
        "scaled_happiness_min_max = scaled_happiness_min_max.drop(columns=['Ladder score in Dystopia_minmax'])\n",
        "scaled_happiness_std = scaled_happiness_std.drop(columns=['Ladder score in Dystopia_standardized'])\n",
        "happiness = happiness.drop(columns=['Ladder score in Dystopia'])"
      ],
      "metadata": {
        "id": "Tnu7QHKHGcwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_happiness_std.head()"
      ],
      "metadata": {
        "id": "Kfnc2YKhGmB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_happiness_min_max.head()"
      ],
      "metadata": {
        "id": "KeWYguXWqah5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4:\n",
        "\n",
        "For each numerical column pair X and Y make a scatter plot of Y vs X with the data as is read in, a scatter plot of Y_minmax vs X_minmax, and a scatter plot of Y_standardized and X_standardized"
      ],
      "metadata": {
        "id": "4njSigaYHbgG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4c9babc"
      },
      "source": [
        "def plot_scatter_pairs(dataframe, title, col_args):\n",
        "\n",
        "    #pd.plotting.scatter_matrix(dataframe, figsize=(15, 15))\n",
        "    #plt.suptitle('Scatter Matrix Plot')\n",
        "    #plt.show()\n",
        "\n",
        "\n",
        "    # Create the scatter plot matrix\n",
        "    sm = pd.plotting.scatter_matrix(dataframe, figsize=(15, 15))\n",
        "\n",
        "    if col_args is not None:\n",
        "      sm = pd.plotting.scatter_matrix(dataframe[col_args], figsize=(15, 15))\n",
        "\n",
        "    # Rotate labels by 45 degrees\n",
        "    for ax in sm.ravel():\n",
        "        ax.xaxis.label.set_rotation(45)\n",
        "        ax.yaxis.label.set_rotation(45)\n",
        "\n",
        "    # Adjust y-axis label position\n",
        "    for i, ax in enumerate(sm[:, 0]): # Only for the first column of subplots (y-axis labels)\n",
        "        ax.yaxis.set_label_coords(-0.8, 0.5)\n",
        "\n",
        "\n",
        "    # Adjust x-axis label position for the bottom row\n",
        "    for i, ax in enumerate(sm[-1, :]): # Only for the bottom row of subplots (x-axis labels)\n",
        "        ax.xaxis.set_label_coords(0.5, -0.2)\n",
        "\n",
        "\n",
        "    plt.suptitle(title, y=0.92, fontsize=16)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_scatter_pairs(happiness, 'World Happiness Report (2021) Scatter Matrix', None)"
      ],
      "metadata": {
        "id": "8hcL4_eaKsWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fig 1:\n",
        "In this figure we are ploting each column data from the world happiness dataset from kaggle in 2021. I was unable to find the dataset from 2024. Here we plot every column against every other column to create our large scatter plot. Each numerical column is plotted against every other column. Each plot is represented by either a scatter plot, or is represented by a histogram in the cases of the data being plotted against itself. We are are abe to see some exact linear relations between some metrics. I am unsure if this is due to some exact scaling law or is due to some kind of data artifacting. However, there seems to be real positive trends in GDP per capita and other metrics seen in the data."
      ],
      "metadata": {
        "id": "fBhHrIOLuJcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_scatter_pairs(scaled_happiness_std, \"World Happiness Report (2021) Scatter Matrix with Standardized Values\", None)"
      ],
      "metadata": {
        "id": "MWKdXgFBLIk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Figure 2:\n",
        "\n",
        "Again, we are ploting each column data from the world happiness dataset from kaggle in 2021. I was unable to find the dataset from 2024. Here we plot every column against every other column to create our large scatter plot. This time we have standardized our data. We are able to see that our data distributions are conserved from the hapiness set, but now range from -2.5 to 2.5."
      ],
      "metadata": {
        "id": "6z-jfCjr0VVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plot_scatter_pairs(scaled_happiness_min_max, \"World Happiness Report (2021) Scatter Matrix with Min Max Values\", None)"
      ],
      "metadata": {
        "id": "mE1C3sNKp8hA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Figure 3:\n",
        "Finally, we are ploting each column data from the world happiness dataset from kaggle in 2021. I was unable to find the dataset from 2024. Here we plot every column against every other column to create our large scatter plot. This time we have min maxed our data. The data now ranges from zero to 1 for all of our data, with our distributions kept in tact."
      ],
      "metadata": {
        "id": "IuZkRLx82Oky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clutering\n",
        "## Task 5:\n",
        "Task 5: Using KMeans clustering, cluster the scaled numerical features (choose either scaling) that are used to calculate the score: 'Family', 'Health (Life Expectancy)', 'Freedom', 'Trust (Government Corruption)', 'Generosity', 'Dystopia Residual' into 3 clusters"
      ],
      "metadata": {
        "id": "_AYWTgeBMUjC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f66fc15"
      },
      "source": [
        "def cluster_happiness_by_columns(original_df, scaled_df, n_clusters, column_indices):\n",
        "    # Get the names of the columns to use for clustering based on numerical indices\n",
        "    scaled_columns = scaled_df.columns\n",
        "    selected_columns = [scaled_columns[i] for i in column_indices if i < len(scaled_columns)]\n",
        "\n",
        "    # Print the names of the selected scaled columns\n",
        "    print(\"Scaled columns used for clustering:\", selected_columns)\n",
        "\n",
        "    # Select the scaled features for clustering\n",
        "    scaled_features = scaled_df[selected_columns]\n",
        "\n",
        "    # Initialize and fit KMeans\n",
        "    kmeans = sklearn.cluster.KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "    original_df['Cluster'] = kmeans.fit_predict(scaled_features)\n",
        "\n",
        "    return original_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_df = happiness\n",
        "scaled_df = scaled_happiness_min_max\n",
        "n_clusters = 3\n",
        "column_indices = [5, 6, 7, 9, 8, 17]\n",
        "cluster_happiness_by_columns(original_df, scaled_df, n_clusters, column_indices)"
      ],
      "metadata": {
        "id": "qZJGw8v4Ox4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 6:\n",
        "Task 6: Make a scatter plot with the cluster (0, 1, or 2) on the X axis, and the Happiness score with its errorbar on the Y axis and, as usual, comment on the figure your what, how, wow"
      ],
      "metadata": {
        "id": "glf4gEmkXPV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create the scatter plot of individual happiness scores by cluster\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Define colors for each cluster\n",
        "colors = {0: 'purple', 1: 'green', 2: 'orange'} # Use a dictionary for mapping cluster to color\n",
        "\n",
        "# Plot each cluster separately\n",
        "for cluster_label in sorted(happiness['Cluster'].unique()):\n",
        "    cluster_data = happiness[happiness['Cluster'] == cluster_label]\n",
        "    plt.scatter(\n",
        "        x=cluster_data['Cluster'],\n",
        "        y=cluster_data['Ladder score'],\n",
        "        color=colors[cluster_label],\n",
        "        label=f'Cluster {cluster_label}',\n",
        "        s=10\n",
        "    )\n",
        "    plt.errorbar(\n",
        "        x=cluster_data['Cluster'],\n",
        "        y=cluster_data['Ladder score'],\n",
        "        yerr=cluster_data['Standard error of ladder score'],\n",
        "        fmt='none',\n",
        "        color=colors[cluster_label],\n",
        "        label=f'Cluster {cluster_label} Score Error'\n",
        "    )\n",
        "\n",
        "# Set labels and title\n",
        "plt.xlabel('Cluster Number')\n",
        "plt.ylabel('Ladder Score')\n",
        "plt.title('Individual Happiness Scores by Cluster (2021)')\n",
        "plt.xticks(sorted(happiness['Cluster'].unique())) # Only show values 0,1,2 on the X axis\n",
        "plt.legend(title='Cluster')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Iae8WSFPXRuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Figure 4:\n",
        "Here, we are plotting our hapiness data that we used KMeans clustering, cluster the scaled numerical features (choose either scaling) that are used to calculate the score: 'Family', 'Health (Life Expectancy)', 'Freedom', 'Trust (Government Corruption)', 'Generosity', 'Dystopia Residual' into 3 clusters, then plotted our three clusters agaisnt our happiness ladder score. Coorisponding error is also included in the plot. The legend outlines as such, with colors coorisponding to each cluster. We are able to see that cluster zero is the elaest happy on average, cluster 1 is happier, and cluster 2 is the most happy on average. Cluster 1 includes the largest range of values, when compared to cluster 0 and 1. There is quite a bit of sharex calues between clusters 1 and the other clusters."
      ],
      "metadata": {
        "id": "JpJzc0Y43h2J"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E8nw96BR5CLj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}